---
title: "STAT429 HW4"
author: "Michael Wieck-Sosa"
date: "11/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(0)
```


```{r}


```

```{r}
#1(i)
n <- 200
X <- runif(n=n,min=0,max=6*pi)
m <- function(x) exp(-0.1*x)*cos(x)
eps <- rnorm(n=n,mean=0,sd=0.1)
Y <- m(X)+eps
plot(X,Y)

xGrid <- seq(-10, 10, l = 500)

# Cite: https://bookdown.org/egarpor/PM-UC3M/npreg-kre.html

NadarayaWatsonEstimator <- function(x, X, Y, h) {

    
  dnormx <- sapply(X, function(X) dnorm((x - X) / h) / h)

  weight <- dnormx / rowSums(dnormx) # get weight for N-W estimator

  drop(weight %*% Y) # scalar product

}

operation <- function(X,x,h){
  matrix(dnorm((x - X) / h) / h)
}

# Bandwidth
h <- 0.5
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1, lwd=2)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2, lwd=2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)

```



```{r}
#1(ii)
h <- 0.1
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)



h <- 2.0
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1, lwd=2)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2, lwd=2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)

```


```{r}
#1(iii)
n = length(X)
# n: sample size
h_seq = seq(from=0.1,to=2.0, by=0.05)
# smoothing bandwidths we are using
CV_err_h = rep(NA,length(h_seq))
for(j in 1:length(h_seq)){
  h_using = h_seq[j]
  CV_err = rep(NA, n)
  for(i in 1:n){
    X_val = X[i]
    Y_val = Y[i]
    # validation set
    X_tr = X[-i]
    Y_tr = Y[-i]
    # training set
    
    # modified so using the correct Nadaraya-Watson

    X_val = rep(X_val,2) # need this because of my function being dumb
    Y_val_predict = NadarayaWatsonEstimator(x = X_val, X = X_tr, Y = Y_tr, h = h_using)

    
    CV_err[i] = (Y_val - Y_val_predict[1])^2
    # we measure the error in terms of difference square
    
  
  }
  CV_err_h[j] = mean(CV_err)
}
CV_err_h
plot(x=h_seq, y=CV_err_h, type="b", lwd=3, col="green",
xlab="Smoothing bandwidth", ylab="LOOCV prediction error")

h_seq[which(CV_err_h == min(CV_err_h))]

```



```{r}
#1(iv)
h <- 0.2
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1, lwd=2)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2,lwd=2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)


```


```{r}
#1(v)
Kreg1 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 0.1)
Kreg2 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 0.5)
Kreg3 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 2.0)
plot(X,Y)

lines(Kreg1, lwd=2, col="red")
lines(Kreg2, lwd=2, col="blue")
lines(Kreg3, lwd=2, col="green")
legend("topright", c("h=0.1","h=0.5","h=2.0"), lwd=6, col=c("red","blue","green"))

```



```{r}

#1(vi)(i)
n <- 200
X <- runif(n=n,min=0,max=6*pi)
m <- function(x) exp(-0.1*x)*cos(x)
eps <- rnorm(n=n,mean=0,sd=0.5)
Y <- m(X)+eps
plot(X,Y)

xGrid <- seq(-10, 10, l = 500)

# Bandwidth
h <- 0.5
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1, lwd=2)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2, lwd=2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)


```



```{r}
#1(vi)(ii)
h <- 0.1
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)



h <- 2.0
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1, lwd=2)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2, lwd=2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)



```



```{r}
#1(vi)(iii)

n = length(X)
# n: sample size
h_seq = seq(from=0.1,to=2.0, by=0.05)
# smoothing bandwidths we are using
CV_err_h = rep(NA,length(h_seq))
for(j in 1:length(h_seq)){
  h_using = h_seq[j]
  CV_err = rep(NA, n)
  for(i in 1:n){
    X_val = X[i]
    Y_val = Y[i]
    # validation set
    X_tr = X[-i]
    Y_tr = Y[-i]
    # training set
    
    # modified so using the correct Nadaraya-Watson
    #Y_val_predict = ksmooth(x=X_tr,y=Y_tr,kernel = "normal",bandwidth=h_using,
    #                        x.points = X_val)
    
    X_val = rep(X_val,2) # need this because of my function being dumb
    Y_val_predict = NadarayaWatsonEstimator(x = X_val, X = X_tr, Y = Y_tr, h = h_using)
    #Y_val_predict$y = Y_val_predict_temp[1]
    
    
    CV_err[i] = (Y_val - Y_val_predict[1])^2
    # we measure the error in terms of difference square
    
  
  }
  CV_err_h[j] = mean(CV_err)
}
CV_err_h
plot(x=h_seq, y=CV_err_h, type="b", lwd=3, col="green",
xlab="Smoothing bandwidth", ylab="LOOCV prediction error")

h_seq[which(CV_err_h == min(CV_err_h))]


```


```{r}
#1(vi)(iv)
h <- 0.6
# Plot data
plot(X, Y,xlim=c(0,9.5))
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, NadarayaWatsonEstimator(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("top", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)

```
```{r}
#1(vi)(v)

#1(v)
Kreg1 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 0.1)
Kreg2 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 0.5)
Kreg3 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 2.0)
plot(X,Y)

lines(Kreg1, lwd=2, col="red")
lines(Kreg2, lwd=2, col="blue")
lines(Kreg3, lwd=2, col="green")
legend("topright", c("h=0.1","h=0.5","h=2.0"), lwd=6, col=c("red","blue","green"))



n = length(X)
# n: sample size
h_seq = seq(from=0.1,to=2.0, by=0.05)
# smoothing bandwidths we are using
CV_err_h = rep(NA,length(h_seq))
for(j in 1:length(h_seq)){
  h_using = h_seq[j]
  CV_err = rep(NA, n)
  for(i in 1:n){
    X_val = X[i]
    Y_val = Y[i]
    # validation set
    X_tr = X[-i]
    Y_tr = Y[-i]
    # training set
    
    # modified so using the correct Nadaraya-Watson
    Y_val_predict = ksmooth(x=X_tr,y=Y_tr,kernel = "normal",bandwidth=h_using,
                            x.points = X_val)


    CV_err[i] = (Y_val - Y_val_predict$y)^2
    # we measure the error in terms of difference square
    
  
  }
  CV_err_h[j] = mean(CV_err,na.rm = TRUE)
}
CV_err_h
plot(x=h_seq, y=CV_err_h, type="b", lwd=2, col="green",
xlab="Smoothing bandwidth", ylab="LOOCV prediction error")

h_seq[which(CV_err_h == min(CV_err_h))]


```





```{r}
#2(i)
path <- "C:/Users/mykul/OneDrive/Documents/stat429/hw4/"
file <- paste(path,"m-mrk2vw.txt",sep="")
data <- read.table(file, sep = '',header = TRUE)
data <- data[,2:7]
head(data)

means <- apply(data, 2, mean) # mean of columns
means



corrMatrix <- cor(data)
corrMatrix


covMatrix <- cov(data)
covMatrix

```



```{r}
#2(ii)
#test for zero crosscor at each lag

library(MTS)
result <- MTS::ccm(data,lag=6,level = T)
result$pvalue



```





```{r}
#3
library(dplyr)
library(vars)

path <- "C:/Users/mykul/OneDrive/Documents/stat429/hw4/"
file <- paste(path,"m-gs1n10.txt",sep="")
data <- read.table(file, sep = '',header = TRUE)
data <- data[4:5]

lag1 <- data %>% mutate_all(lag)

change <- data - lag1
change <- change[-1,]

lagselect <- VARselect(change, lag.max = 12, type = "both")
lagselect$selection
lagselect$criteria


# Estimate reduced form VAR
var_est <- VAR(change, p = 6, type = "none")
var_est


#A-model
# Estimate structural coefficients
# Estimation method scoring
a <- diag(1, 2)
a[lower.tri(a)] <- NA
svar_est_a1 <- SVAR(var_est,estmethod="scoring",Amat = a, max.iter = 1000)
svar_est_a1
solve(svar_est_a1$A)


## Estimation method direct
a <- diag(1, 2)
a[lower.tri(a)] <- NA
svar_est_a2 <- SVAR(var_est,estmethod="direct",Amat = a,hessian=TRUE,
                   method="BFGS")
svar_est_a2
solve(svar_est_a2$A)


#B-model
# Create structural matrix with restrictions
b <- diag(1, 2)
b[lower.tri(b)] <- NA
# Estimate
svar_est_b <- SVAR(var_est, Bmat = b)
# Show result
svar_est_b



# for interpreting
irf_res <- irf(svar_est_a1)
plot(irf_res)

SVARfevd <- fevd(svar_est_a1, n.ahead = 10)
SVARfevd
plot(SVARfevd)

```






```{r}
#4(i)
library(aTSA)
path <- "C:/Users/mykul/OneDrive/Documents/stat429/hw4/"
file <- paste(path,"m-gs1n3-5304.txt",sep="")
data <- read.table(file, sep = '',header = FALSE)
data <- data[0:2]

colnames(data) <- c("1YRM","3YRM")

lagselect <- VARselect(data, lag.max = 12, type = "both")
lagselect$selection
lagselect$criteria

# Estimate reduced form VAR
var_est1 <- VAR(data, p = 6, type = "none")
var_est1

```

```{r}
#4(ii)
irf_res <- irf(var_est1)
plot(irf_res)

irf_res

```


```{r}
#4(iii)
library(zoo)
library(urca)
library(aTSA)
path <- "C:/Users/mykul/OneDrive/Documents/stat429/hw4/"
file <- paste(path,"m-gs1n3-5304.txt",sep="")
data <- read.table(file, sep = '',header = FALSE)
data <- data[0:2]
colnames(data) <- c("1YRM","3YRM")

coint.test(data[,"1YRM"],data[,"3YRM"])

jotest=ca.jo(data, type="trace", K=6, ecdet="none", spec="longrun")
summary(jotest)


```
```{r}
#4(iv)
# Load package
library(tsDyn)
library(bvartools)

# Estimate
est_tsdyn <- VECM(data, lag = 6, r = 1, include = "none", estim = "ML")

# Print results
summary(est_tsdyn)


```










